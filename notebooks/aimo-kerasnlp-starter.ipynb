{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":73231,"databundleVersionId":8133715,"sourceType":"competition"},{"sourceId":8009768,"sourceType":"datasetVersion","datasetId":4717827},{"sourceId":8076106,"sourceType":"datasetVersion","datasetId":4766079},{"sourceId":27825,"sourceType":"modelInstanceVersion","modelInstanceId":22009}],"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/awsaf49/aimo-kerasnlp-starter?scriptVersionId=172896175\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"<center><img src=\"https://keras.io/img/logo-small.png\" alt=\"Keras logo\" width=\"100\"><br/>\nThis starter notebook is provided by the Keras team.</center>","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"# AI Math Olympiad with [KerasNLP](https://github.com/keras-team/keras-nlp) and [Keras](https://github.com/keras-team/keras)\n\n<div align=\"center\">\n    <img src=\"https://i.ibb.co/9rx4pbX/AIMO.png\">\n</div>\n\nIn this competition, we aim is to build AI models that can solve tough math problems, in other words, creating LLM models capable of solving Math Olympiad problems. This notebook will guide you through the process of fine-tuning the **Gemma** LLM model with LoRA to solve math problems using KerasNLP. With KerasNLP, fine-tuning with LoRA becomes straightforward with just a few lines of code.\n\n**Did you know:**: This notebook is backend-agnostic? Which means it supports TensorFlow, PyTorch, and JAX backends. However, the best performance can be achieved with `JAX`. KerasNLP and Keras enable the choice of preferred backend. Explore further details on [Keras](https://keras.io/keras_3/).\n\n**Note**: For a deeper understanding of KerasNLP, refer to the [KerasNLP guides](https://keras.io/keras_nlp/).","metadata":{}},{"cell_type":"markdown","source":"# Install Libraries\n\nWe need to install latest KerasNLP to load Gemma 1.1 model. As we don't have access to internet during inference, we will be installing this library from our local files.","metadata":{}},{"cell_type":"code","source":"!pip install -q /kaggle/input/keras-lib-dataset/keras_nlp-0.9.2-py3-none-any.whl --no-deps","metadata":{"execution":{"iopub.status.busy":"2024-04-19T14:16:46.210564Z","iopub.execute_input":"2024-04-19T14:16:46.211194Z","iopub.status.idle":"2024-04-19T14:16:49.367611Z","shell.execute_reply.started":"2024-04-19T14:16:46.211161Z","shell.execute_reply":"2024-04-19T14:16:49.366521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Import Libraries ","metadata":{}},{"cell_type":"code","source":"import os\nos.environ[\"KERAS_BACKEND\"] = \"jax\" # you can also use tensorflow or torch\nos.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"0.9\" # avoid memory fragmentation on JAX backend.\n\nimport keras\nimport keras_nlp\n\nimport numpy as np\nimport pandas as pd\nfrom tqdm.notebook import tqdm\ntqdm.pandas() # progress bar for pandas\n\nimport plotly.graph_objs as go\nimport plotly.express as px\nfrom IPython.display import display, Markdown","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-04-19T14:16:49.369911Z","iopub.execute_input":"2024-04-19T14:16:49.37027Z","iopub.status.idle":"2024-04-19T14:17:04.207619Z","shell.execute_reply.started":"2024-04-19T14:16:49.370236Z","shell.execute_reply":"2024-04-19T14:17:04.206485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Configuration","metadata":{}},{"cell_type":"code","source":"class CFG:\n    seed = 42\n    dataset_path = \"/kaggle/input/ai-mathematical-olympiad-prize\"\n    preset = \"gemma_1.1_instruct_2b_en\" # name of pretrained Gemma\n    sequence_length = 512 # max size of input sequence for training\n    batch_size = 1 # size of the input batch in training\n    epochs = 1 # number of epochs to train","metadata":{"execution":{"iopub.status.busy":"2024-04-19T14:17:04.20896Z","iopub.execute_input":"2024-04-19T14:17:04.209742Z","iopub.status.idle":"2024-04-19T14:17:04.214649Z","shell.execute_reply.started":"2024-04-19T14:17:04.20969Z","shell.execute_reply":"2024-04-19T14:17:04.213705Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Reproducibility \nSets value for random seed to produce similar result in each run.","metadata":{}},{"cell_type":"code","source":"keras.utils.set_random_seed(CFG.seed)","metadata":{"execution":{"iopub.status.busy":"2024-04-19T14:17:04.217161Z","iopub.execute_input":"2024-04-19T14:17:04.218104Z","iopub.status.idle":"2024-04-19T14:17:04.251849Z","shell.execute_reply.started":"2024-04-19T14:17:04.21807Z","shell.execute_reply":"2024-04-19T14:17:04.250798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data\n\nNo training data is provided in this competition; in other words, we can use any openly available datasets for this competition. In this notebook, we will use a modified **Math** dataset which I have compiled to have a `Question-Solution-Answer` format.\n\n**Data Format:**\n\nThese datasets include:\n- `problem`: The math problem in LaTeX format.\n- `solution`: Step-by-step solution to this problem.\n- `answer`: Final answer of the solution which will be the ground truth for this competition.\n- `level`: Difficulty of the problem.\n- `type`: The category of the problem.\n\n> This dataset comes with its own train test split. However, we will merge them both and use them for fine-tuning. You are welcome to use them for trainining and validation separately. Also to reduce the training time we will only be training on the first`1000` samples. You are welcome to train on the full data.","metadata":{}},{"cell_type":"code","source":"df1 = pd.read_csv(\"/kaggle/input/math-qsa-dataset/train.csv\")\ndf2 = pd.read_csv(\"/kaggle/input/math-qsa-dataset/test.csv\")\ndf = pd.concat([df1, df2], axis=0)\ndf = df[:1000] # take first 1000 samples\ndf.head(2)","metadata":{"execution":{"iopub.status.busy":"2024-04-19T14:17:04.254546Z","iopub.execute_input":"2024-04-19T14:17:04.25522Z","iopub.status.idle":"2024-04-19T14:17:04.518786Z","shell.execute_reply.started":"2024-04-19T14:17:04.255186Z","shell.execute_reply":"2024-04-19T14:17:04.517817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Filter Data\n\nThe Math dataset contains various problems, but not all of them are suitable for this competition. More specifically, this competition requires a `non-negative integer` answer, while the Math dataset includes problems with different types of answers such as integers, floats, fractions, matrices, etc. In this notebook, we will only use those problems whose answers are non-negative integers and filter out the rest.","metadata":{}},{"cell_type":"code","source":"def is_integer(text):\n    try:\n        if int(text) >= 0:\n            return True\n        else:\n            return False\n    except ValueError:\n        return False\n    \ndf[\"is_integer\"] = df.answer.map(is_integer)\ndf = df[df.is_integer].reset_index(drop=True)\ndf.head(2)","metadata":{"execution":{"iopub.status.busy":"2024-04-19T14:17:04.51992Z","iopub.execute_input":"2024-04-19T14:17:04.521541Z","iopub.status.idle":"2024-04-19T14:17:04.541858Z","shell.execute_reply.started":"2024-04-19T14:17:04.521503Z","shell.execute_reply":"2024-04-19T14:17:04.540849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prompt Engineering\n\nWe will be using below simple prompt template we'll use to create problem-solution-answer trio to feed the model. This template will help the model to follow instruction and respond accurately. You can explore more advanced prompt templates for better results. \n\n```\nRole:\nYou are an advanced AI system with exceptional mathematical reasoning and problem-solving capabilities, specifically designed to solve tricky math problems (whose answer is a non-negative integer) written in LaTeX format from the AI Mathematical Olympiad (AIMO) competition. Your task is to accurately analyze and solve intricate mathematical problems, demonstrating a deep understanding of mathematical concepts and a strong ability to apply logical reasoning strategies.\n\nInstruction:\n1. Carefully read and comprehend the problem statement provided in the \"Problem\" section.\n2. In the \"Solution\" section, provide a solution of the problem with detailed explanation of your logical reasoning process. Keep in mind that answer must be a non-negative integer number.\n3. At the end, create a \"Answer\" section where you will state only the final numerical or algebraic answer, without any additional text or narrative.\n\nProblem:\n...\n\nSolution:\n...\n\nAnswer:\n...\n```","metadata":{}},{"cell_type":"code","source":"template = \"\"\"Role:\\nYou are an advanced AI system with exceptional mathematical reasoning and problem-solving capabilities, specifically designed to solve tricky math problems (whose answer is a non-negative integer) written in LaTeX format from the AI Mathematical Olympiad (AIMO) competition. Your task is to accurately analyze and solve intricate mathematical problems, demonstrating a deep understanding of mathematical concepts and a strong ability to apply logical reasoning strategies.\\n\\nInstruction:\n1. Carefully read and comprehend the problem statement provided in the \"Problem\" section.\n2. In the \"Solution\" section, provide a solution of the problem with detailed explanation of your logical reasoning process. Keep in mind that answer must be a non-negative integer number.\n3. At the end, create a \"Answer\" section where you will state only the final numerical or algebraic answer, without any additional text or narrative.\\n\\nProblem:\\n{problem}\\n\\nSolution:\\n{solution}\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-04-19T14:17:04.543106Z","iopub.execute_input":"2024-04-19T14:17:04.543461Z","iopub.status.idle":"2024-04-19T14:17:04.551334Z","shell.execute_reply.started":"2024-04-19T14:17:04.543426Z","shell.execute_reply":"2024-04-19T14:17:04.550333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[\"prompt\"] = df.progress_apply(lambda row: template.format(problem=row.problem,\n                                                             solution=f\"{row.solution}\\n\\nAnswer:\\n{row.answer}\"),\n                                                             axis=1)\ndata = df.prompt.tolist()","metadata":{"execution":{"iopub.status.busy":"2024-04-19T14:17:04.552813Z","iopub.execute_input":"2024-04-19T14:17:04.553121Z","iopub.status.idle":"2024-04-19T14:17:04.612218Z","shell.execute_reply.started":"2024-04-19T14:17:04.553094Z","shell.execute_reply":"2024-04-19T14:17:04.611283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's examine a sample prompt. As the answers in our dataset are curated with **markdown** format, we will render the sample using `Markdown()` to properly visualize the formatting.","metadata":{}},{"cell_type":"markdown","source":"## Check Sample","metadata":{}},{"cell_type":"code","source":"def colorize_text(text):\n    for word, color in zip([\"Role\", \"Instruction\", \"Problem\", \"Solution\", \"Answer\"],\n                           [\"blue\", \"yellow\", \"red\", \"cyan\", \"green\"]):\n        text = text.replace(f\"{word}:\", f\"\\n\\n**<font color='{color}'>{word}:</font>**\")\n    return text","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-04-19T14:17:04.613356Z","iopub.execute_input":"2024-04-19T14:17:04.613767Z","iopub.status.idle":"2024-04-19T14:17:04.619086Z","shell.execute_reply.started":"2024-04-19T14:17:04.613736Z","shell.execute_reply":"2024-04-19T14:17:04.618192Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Sample 1","metadata":{}},{"cell_type":"code","source":"# Take a random sample\nsample = data[12]\n\n# Give colors to Instruction, Response and Category\nsample = colorize_text(sample)\n\n# Show sample in markdown\ndisplay(Markdown(sample))","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-04-19T14:40:00.730097Z","iopub.execute_input":"2024-04-19T14:40:00.730915Z","iopub.status.idle":"2024-04-19T14:40:00.737837Z","shell.execute_reply.started":"2024-04-19T14:40:00.730882Z","shell.execute_reply":"2024-04-19T14:40:00.736791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Sample 2","metadata":{}},{"cell_type":"code","source":"# Take a random sample\nsample = data[32]\n\n# Give colors to Instruction, Response and Category\nsample = colorize_text(sample)\n\n# Show sample in markdown\ndisplay(Markdown(sample))","metadata":{"execution":{"iopub.status.busy":"2024-04-19T14:17:04.633024Z","iopub.execute_input":"2024-04-19T14:17:04.633294Z","iopub.status.idle":"2024-04-19T14:17:04.640136Z","shell.execute_reply.started":"2024-04-19T14:17:04.633271Z","shell.execute_reply":"2024-04-19T14:17:04.639192Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modeling\n\n<div align=\"center\"><img src=\"https://i.ibb.co/Bqg9w3g/Gemma-Logo-no-background.png\" width=\"300\"></div>\n\n**Gemma** is a collection of advanced open LLMs developed by **Google DeepMind** and other **Google teams**, derived from the same research and technology behind the **Gemini** models. They can be integrated into applications and run on various platforms including mobile devices and hosted services. Developers can customize Gemma models using tuning techniques to enhance their performance for specific tasks, offering more targeted and efficient generative AI solutions beyond text generation.\n\nGemma models are available in several sizes so we can build generative AI solutions based on your available computing resources, the capabilities you need, and where you want to run them.\n\n| Parameters size | Tuned versions    | Intended platforms                 | Preset                 |\n|-----------------|-------------------|------------------------------------|------------------------|\n| 2B              | Pretrained        | Mobile devices and laptops         | `gemma_2b_en`          |\n| 2B              | Instruction tuned | Mobile devices and laptops         | `gemma_1.1_instruct_2b_en` |\n| 2B              | Pretrained        | Code Completion in Mobile Device   | `code_gemma_2b_en` |\n| 7B              | Pretrained        | Desktop computers and small servers| `gemma_7b_en`          |\n| 7B              | Instruction tuned | Desktop computers and small servers| `gemma_1.1_instruct_7b_en` |\n| 7B              | Instruction tuned | Code Completion in Desktop computers| `code_gemma_7b_en` |\n\nIn this notebook, we will utilize the `Gemma 1.1 2b-it` model from KerasNLP's pretrained models to solve the math olympiad questions. We are using the \"Instruction tuned\" model instead of the \"Pretrained\" one because it is easier for the model to fine-tune on the prepared dataset. \n\nTo explore other available models, you can simply adjust the `preset` value in the `CFG` (config). You can find a list of other pretrained models on the [KerasNLP website](https://keras.io/api/keras_nlp/models/).","metadata":{}},{"cell_type":"markdown","source":"## Gemma Causal LM\n\nThe code below will build an end-to-end Gemma model for causal language modeling (hence the name `GemmaCausalLM`). A causal language model (LM) predicts the next token based on previous tokens. This task setup can be used to train the model unsupervised on plain text input or to autoregressively generate plain text similar to the data used for training. This task can be used for pre-training or fine-tuning a Gemma model simply by calling `fit()`.\n\nThis model has a `generate()` method, which generates text based on a prompt. The generation strategy used is controlled by an additional sampler argument on `compile()`. You can recompile the model with different `keras_nlp.samplers` objects to control the generation. By default, `\"greedy\"` sampling will be used.\n\n> The `from_preset` method instantiates the model from a preset architecture and weights.","metadata":{}},{"cell_type":"code","source":"gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(CFG.preset)\ngemma_lm.summary()","metadata":{"execution":{"iopub.status.busy":"2024-04-19T14:17:04.641286Z","iopub.execute_input":"2024-04-19T14:17:04.641565Z","iopub.status.idle":"2024-04-19T14:18:09.093769Z","shell.execute_reply.started":"2024-04-19T14:17:04.641542Z","shell.execute_reply":"2024-04-19T14:18:09.092709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Gemma LM Preprocessor\n\nAn important part of the Gemma model is the **Preprocessor** layer, which under the hood uses **Tokenizer**.\n\n**What it does:** The preprocessor takes input strings and transforms them into a dictionary (`token_ids`, `padding_mask`) containing preprocessed tensors. This process starts with tokenization, where input strings are converted into sequences of token IDs.\n\n**Why it's important:** Initially, raw text data is complex and challenging for modeling due to its high dimensionality. By converting text into a compact set of tokens, such as transforming `\"The quick brown fox\"` into `[\"the\", \"qu\", \"##ick\", \"br\", \"##own\", \"fox\"]`, we simplify the data. Many models rely on special tokens and additional tensors to understand input. These tokens help divide input and identify padding, among other tasks. Making all sequences the same length through padding boosts computational efficiency, making subsequent steps smoother.\n\nExplore the following pages to access the available preprocessing and tokenizer layers in **KerasNLP**:\n- [Preprocessing](https://keras.io/api/keras_nlp/preprocessing_layers/)\n- [Tokenizers](https://keras.io/api/keras_nlp/tokenizers/)","metadata":{}},{"cell_type":"code","source":"x, y, sample_weight = gemma_lm.preprocessor(data[0:2])","metadata":{"execution":{"iopub.status.busy":"2024-04-19T14:18:09.095219Z","iopub.execute_input":"2024-04-19T14:18:09.095582Z","iopub.status.idle":"2024-04-19T14:18:09.443047Z","shell.execute_reply.started":"2024-04-19T14:18:09.095551Z","shell.execute_reply":"2024-04-19T14:18:09.442022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This preprocessing layer will take in batches of strings, and return outputs in a `(x, y, sample_weight)` format, where the `y` label is the next token id in the `x` sequence.\n\nFrom the code below, we can see that, after the preprocessor, the data shape is `(num_samples, sequence_length)`.","metadata":{}},{"cell_type":"code","source":"# Display the shape of each processed output\nfor k, v in x.items():\n    print(k, \":\", v.shape)","metadata":{"execution":{"iopub.status.busy":"2024-04-19T14:18:09.444468Z","iopub.execute_input":"2024-04-19T14:18:09.444834Z","iopub.status.idle":"2024-04-19T14:18:09.450519Z","shell.execute_reply.started":"2024-04-19T14:18:09.444801Z","shell.execute_reply":"2024-04-19T14:18:09.449607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference before Fine-Tuning\n\nBefore we do fine-tuning, let's see how Gemma model responds with some prepared prompts.\n\n> As this model is not yet fine-tuned for instruction, you will notice that the model's responses are inaccurate.","metadata":{}},{"cell_type":"markdown","source":"## Sample 1","metadata":{}},{"cell_type":"code","source":"# Take one sample\nrow = df.iloc[12]\n\n# Generate Prompt using template\nprompt = template.format(\n    problem=row.problem,\n    solution=\"\",\n)\n\n# Infer\noutput = gemma_lm.generate(prompt, max_length=1024)\n\n# Colorize\noutput = colorize_text(output)\n\n# Display in markdown\ndisplay(Markdown(output))\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-04-19T14:18:09.451781Z","iopub.execute_input":"2024-04-19T14:18:09.452566Z","iopub.status.idle":"2024-04-19T14:18:26.254776Z","shell.execute_reply.started":"2024-04-19T14:18:09.452534Z","shell.execute_reply":"2024-04-19T14:18:26.25356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Sample 2","metadata":{}},{"cell_type":"code","source":"# Take one sample\nrow = df.iloc[32]\n\n# Generate Prompt using template\nprompt = template.format(\n    problem=row.problem,\n    solution=\"\"\n)\n\n# Infer\noutput = gemma_lm.generate(prompt, max_length=1024)\n\n# Colorize\noutput = colorize_text(output)\n\n# Display in markdown\ndisplay(Markdown(output))\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-04-19T14:18:26.256154Z","iopub.execute_input":"2024-04-19T14:18:26.256488Z","iopub.status.idle":"2024-04-19T14:18:27.901428Z","shell.execute_reply.started":"2024-04-19T14:18:26.25646Z","shell.execute_reply":"2024-04-19T14:18:27.900201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Fine-tuning with LoRA\n\nTo get better responses from the model, we will fine-tune the model with Low Rank Adaptation (LoRA).\n\n**What exactly is LoRA?**\n\nLoRA is a method used to fine-tune large language models (LLMs) in an efficient way. It involves freezing the weights of the LLM and injecting trainable rank-decomposition matrices.\n\nImagine in an LLM, we have a pre-trained dense layer, represented by a $d \\times d$ weight matrix, denoted as $W_0$. We then initialize two additional dense layers, labeled as $A$ and $B$, with shapes $d \\times r$ and $r \\times d$, respectively. Here, $r$ denotes the rank, which is typically **much smaller than** $d$. Prior to LoRA, the model's output was computed using the equation $output = W_0 \\cdot x + b_0$, where $x$ represents the input and $b_0$ denotes the bias term associated with the original dense layer, which remains frozen. After applying LoRA, the equation becomes $output = (W_0 \\cdot x + b_0) + (B \\cdot A \\cdot x)$, where $A$ and $B$ denote the trainable rank-decomposition matrices that have been introduced.\n\n<center><img src=\"https://i.ibb.co/DWsbhLg/LoRA.png\" width=\"300\"><br/>\nCredit: <a href=\"https://arxiv.org/abs/2106.09685\">LoRA: Low-Rank Adaptation of Large Language Models</a> Paper</center>\n\n\nIn the LoRA paper, $A$ is initialized with $\\mathcal{N} (0, \\sigma^2)$ and $B$ with $0$, where $\\mathcal{N}$ denotes the normal distribution, and $\\sigma^2$ is the variance.\n\n**Why does LoRA save memory?**\n\nEven though we're adding more layers to the model with LoRA, it actually helps save memory. This is because the smaller layers (A and B) have fewer parameters to learn compared to the big model and fewer trainable parameters mean fewer optimizer variables to store. So, even though the overall model might seem bigger, it's actually more efficient in terms of memory usage. \n\n> This notebook uses a LoRA rank of `4`. A higher rank means more detailed changes are possible, but also means more trainable parameters.","metadata":{}},{"cell_type":"code","source":"# Enable LoRA for the model and set the LoRA rank to 4.\ngemma_lm.backbone.enable_lora(rank=4)\ngemma_lm.summary()","metadata":{"execution":{"iopub.status.busy":"2024-04-19T14:18:27.902999Z","iopub.execute_input":"2024-04-19T14:18:27.903949Z","iopub.status.idle":"2024-04-19T14:18:28.380656Z","shell.execute_reply.started":"2024-04-19T14:18:27.90391Z","shell.execute_reply":"2024-04-19T14:18:28.379767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Notice** that, the number of trainable parameters is reduced from ~$2.5$ billions to ~$1.3$ millions after enabling LoRA.","metadata":{}},{"cell_type":"markdown","source":"## Training","metadata":{}},{"cell_type":"code","source":"# Limit the input sequence length to 512 (to control memory usage).\ngemma_lm.preprocessor.sequence_length = CFG.sequence_length \n\n# Compile the model with loss, optimizer, and metric\ngemma_lm.compile(\n    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    optimizer=keras.optimizers.Adam(learning_rate=2e-5),\n    weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],\n)\n\n# Train model\ngemma_lm.fit(data, epochs=CFG.epochs, batch_size=CFG.batch_size)","metadata":{"execution":{"iopub.status.busy":"2024-04-19T14:18:28.382538Z","iopub.execute_input":"2024-04-19T14:18:28.382975Z","iopub.status.idle":"2024-04-19T14:27:05.615248Z","shell.execute_reply.started":"2024-04-19T14:18:28.382941Z","shell.execute_reply":"2024-04-19T14:27:05.614308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference after fine-tuning\n\nLet's see how our fine-tuned model responds to the same questions we asked before fine-tuning the model.","metadata":{}},{"cell_type":"markdown","source":"## Sample 1","metadata":{}},{"cell_type":"code","source":"# Take one sample\nrow = df.iloc[12]\n\n# Generate Prompt using template\nprompt = template.format(\n    problem=row.problem,\n    solution=\"\"\n)\n\n# Infer\noutput = gemma_lm.generate(prompt, max_length=1024)\n\n# Colorize\noutput = colorize_text(output)\n\n# Display in markdown\ndisplay(Markdown(output))","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-04-19T14:40:17.963335Z","iopub.execute_input":"2024-04-19T14:40:17.963709Z","iopub.status.idle":"2024-04-19T14:40:22.677954Z","shell.execute_reply.started":"2024-04-19T14:40:17.963679Z","shell.execute_reply":"2024-04-19T14:40:22.677051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Sample 2","metadata":{}},{"cell_type":"code","source":"# Take one sample\nrow = df.iloc[32]\n\n# Generate Prompt using template\nprompt = template.format(\n    problem=row.problem,\n    solution=\"\"\n)\n\n# Infer\noutput = gemma_lm.generate(prompt, max_length=1024)\n\n# Colorize\noutput = colorize_text(output)\n\n# Display in markdown\ndisplay(Markdown(output))\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-04-19T14:38:50.003518Z","iopub.execute_input":"2024-04-19T14:38:50.004424Z","iopub.status.idle":"2024-04-19T14:38:53.287786Z","shell.execute_reply.started":"2024-04-19T14:38:50.004389Z","shell.execute_reply":"2024-04-19T14:38:53.286864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# AIMO Data\n\nSo far we have inferred our model on **Math** dataset but now let's see how our model perform on AIMO (competition) dataset.","metadata":{}},{"cell_type":"markdown","source":"## Utilities","metadata":{}},{"cell_type":"code","source":"import re\n\n# Extract answer from model response\ndef get_answer(text):\n    try:\n        answer = re.search(r'Answer:\\s*([\\s\\S]+)', text).group(1).strip()\n        answer = answer.replace(\",\",\"\")\n        if is_integer(answer):\n            return int(answer)%1000\n        else:\n            return 0\n    except:\n        return 0\n    \n    \ndef infer(df):\n    preds = []\n    for i in tqdm(range(len(df))):\n        row = df.iloc[i]\n\n        # Generate Prompt using template\n        prompt = template.format(\n            problem=row.problem,\n            solution=\"\"\n        )\n\n        # Infer\n        output = gemma_lm.generate(prompt, max_length=1024)\n        pred = get_answer(output)\n\n        # Store predictions\n        preds.append([row.id, pred])\n        if \"answer\" in row:\n            preds[-1] += [row.answer]\n    return preds","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-04-19T14:27:25.369058Z","iopub.execute_input":"2024-04-19T14:27:25.369306Z","iopub.status.idle":"2024-04-19T14:27:25.377883Z","shell.execute_reply.started":"2024-04-19T14:27:25.369284Z","shell.execute_reply":"2024-04-19T14:27:25.376902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Inference on AIMO Data","metadata":{}},{"cell_type":"code","source":"aimo_df = pd.read_csv(f\"{CFG.dataset_path}/train.csv\")\ntrain_preds = infer(aimo_df)\ntrain_pred_df = pd.DataFrame(train_preds, columns=[\"id\", \"prediction\", \"answer\"])\ntrain_pred_df","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2024-04-19T14:27:25.378995Z","iopub.execute_input":"2024-04-19T14:27:25.379258Z","iopub.status.idle":"2024-04-19T14:28:06.239544Z","shell.execute_reply.started":"2024-04-19T14:27:25.379236Z","shell.execute_reply":"2024-04-19T14:28:06.238644Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"markdown","source":"## Infer on Test Data","metadata":{}},{"cell_type":"code","source":"test_df = pd.read_csv(f\"{CFG.dataset_path}/test.csv\")\ntest_preds = infer(test_df)","metadata":{"execution":{"iopub.status.busy":"2024-04-19T14:28:06.240768Z","iopub.execute_input":"2024-04-19T14:28:06.241057Z","iopub.status.idle":"2024-04-19T14:28:09.60068Z","shell.execute_reply.started":"2024-04-19T14:28:06.241033Z","shell.execute_reply":"2024-04-19T14:28:09.599788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Prepare Submission File\n\nWhile preparing the submission file, we must keep in mind that, the answer must be between `0-999`. This can easily handled by using `remainder (%)` operation. For this notebook, this step is already applied in the inference stage while extracting `answer` from `solution`. So, we don't need to separately apply it heer.","metadata":{}},{"cell_type":"code","source":"sub_df = pd.DataFrame(test_preds, columns=[\"id\", \"answer\"])\nsub_df.to_csv(\"submission.csv\",index=False,header=True)\nsub_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-04-19T14:28:09.602126Z","iopub.execute_input":"2024-04-19T14:28:09.602515Z","iopub.status.idle":"2024-04-19T14:28:09.617016Z","shell.execute_reply.started":"2024-04-19T14:28:09.602482Z","shell.execute_reply":"2024-04-19T14:28:09.615904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conclusion\n\nWe can see that after fine-tuning, the model is following instructions more accurately. However, it may still struggle to solve problems accurately, which can be attributed to its small size. Nevertheless, there is ample room for improvement. Here are some tips to enhance performance:\n\n- Train on the full data instead of first `1000` samples.\n- Try using the non-instruction-tuned version of Gemma.\n- Increase the `sequence_length`.\n- Experiment with advanced prompt engineering techniques.\n- Implement augmentation to increase the number of samples.\n- Utilize a learning rate scheduler.","metadata":{}},{"cell_type":"markdown","source":"# Reference\n* [Fine-tune Gemma models in Keras using LoRA](https://www.kaggle.com/code/nilaychauhan/fine-tune-gemma-models-in-keras-using-lora)\n* [Parameter-efficient fine-tuning of GPT-2 with LoRA](https://keras.io/examples/nlp/parameter_efficient_finetuning_of_gpt2_with_lora/)\n* [Gemma - KerasNLP](https://keras.io/api/keras_nlp/models/gemma/)","metadata":{}}]}